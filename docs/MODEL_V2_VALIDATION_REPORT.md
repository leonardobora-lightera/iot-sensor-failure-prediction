# Model v2 FIELD-only - Validation Report

**Document Version:** 1.1 (Revised Nov 18, 2025)  
**Date:** November 14, 2025  
**Model Version:** CatBoost v2.0 FIELD-only (trained Nov 13, 2025)  
**Validation Team:** Leonardo Costa (Data Science)  
**Status:** âš ï¸ **EXPERIMENTAL VALIDATION - MVP/POC READY**

---

## âš ï¸ IMPORTANT DISCLAIMER

**This validation report documents experiments conducted on November 14, 2025, BEFORE the Discovery 0 data cleanup was fully understood.**

**Critical Context:**
- **Experiments 1-3 used 789-device MIXED dataset** (FIELD + FACTORY lifecycle data)
- **Threshold experiment used 237-device test set** from pre-cleanup split
- **TRUE v2.0 FIELD-only baseline:** 57.1% precision/recall (from metadata.json, 229 FIELD-only test set)
- **Experimental metrics (90.9% precision, 71.4% recall) NOT applicable to final v2.0 model**

**Purpose of this document:**
- Documents research methodology and validation approach
- Demonstrates feature importance analysis and threshold optimization techniques
- Provides insights for future model iterations (FASE 3)
- **NOT a production validation report** - use metadata.json for true v2.0 performance

**For accurate v2.0 metrics, refer to:** `models/catboost_pipeline_v2_metadata.json`

---

## ðŸ“‹ Executive Summary

This report documents **experimental validation** of the IoT Sensor Failure Prediction Model v2 conducted on November 14, 2025. Three experiments assessed model behavior, feature contribution, and threshold optimization.

### âš ï¸ Key Limitation

**Experiments used pre-cleanup MIXED dataset (789 devices FIELD+FACTORY).** Metrics reported here DO NOT reflect true v2.0 FIELD-only performance.

**TRUE v2.0 Baseline (from metadata.json):**
- **Precision: 57.1%** (8 TP, 6 FP on 229 FIELD-only test set)
- **Recall: 57.1%** (8/14 critical devices detected)
- **F1-Score: 0.571**
- **ROC-AUC: 0.9186**

### Research Findings (Experimental Dataset)

âœ… **Feature Importance Validated**
- Critical device detection capability demonstrated via outlier analysis
- Temporal feature (`days_since_last_message`) ranks #8/30 (4.73% importance)
- Optical features dominate (31.39% total contribution)

âœ… **Experimental Metrics (237-device MIXED test set, Threshold 0.60)**
- **Precision: 90.9%** (âš ï¸ NOT applicable to v2.0 FIELD-only)
- **Recall: 71.4%** (âš ï¸ NOT applicable to v2.0 FIELD-only)
- **F1-Score: 0.800** (âš ï¸ Experimental only)

âœ… **Methodology Insights**
- Threshold optimization technique validated for future iterations
- Outlier analysis correlates with model probabilities
- Signal variance features (rsrp_std, snr_std) show predictive value

### Validation Experiments Conducted

1. **Critical Devices Analysis** - Outlier detection via z-scores (3 devices analyzed)
2. **Feature Importance Analysis** - CatBoost native importance ranking (30 features)
3. **Threshold Adjustment Experiment** - Precision-recall optimization (7 thresholds tested)

---

## ðŸŽ¯ Background & Objectives

### FASE 2 Context

Model v2 represents the second phase of development, addressing critical issues discovered in v1:

**Key Changes from v1:**
- **FACTORY Data Removal:** 362,343 messages (31.8%) filtered to eliminate lifecycle contamination
- **Temporal Feature Addition:** `days_since_last_message` added as 30th feature
- **Dataset Purification:** 762 FIELD-only devices (vs 789 mixed in v1)
- **Performance Trade-off:** -21.5% recall BUT +6.6% AUC (0.8621 â†’ 0.9186)

**Discovery 0 - Lifecycle Contamination:**
- Device 861275072515287 false positive caused by FACTORY (lab testing) messages
- Solution: MODE='FIELD' filter removed pre-deployment testing data

### Validation Objectives

1. **Model Health Assessment** - Verify v2 detects genuine failures (not artifacts)
2. **Temporal Feature Validation** - Confirm `days_since_last_message` adds predictive value
3. **Threshold Optimization** - Find optimal decision boundary for production

---

## ðŸ”¬ Methodology

### Experiment 1: Critical Devices Outlier Analysis

**Objective:** Validate if model detections correspond to real device degradation

**Method:**
1. Load 640 devices from batch upload (payload_aws_BORA_transformed_v2.csv)
2. Extract 3 devices detected as CRITICAL by model v2:
   - Device 866207059671895 (99.7% probability)
   - Device 861275072514504 (82.1% probability)
   - Device 861275072341072 (59.8% probability)
3. Calculate z-scores for all 30 features vs baseline (640 devices)
4. Identify outliers: |z-score| > 2Ïƒ (95% confidence interval)
5. Classify as: LEGITIMATE (â‰¥5 outliers), MODERATE (2-4), WEAK (<2)

**Tools:** Python script `analyze_critical_devices.py`

### Experiment 2: Feature Importance Analysis

**Objective:** Quantify contribution of each feature, especially `days_since_last_message`

**Method:**
1. Load trained CatBoost model (catboost_pipeline_v2_field_only.pkl)
2. Extract `model.feature_importances_` (native CatBoost algorithm)
3. Rank features by importance (1-30)
4. Group into categories (Optical, Messaging, Battery, Temperature, Signal)
5. Visualize top 15 features and category contributions

**Tools:** Python script `feature_importance_analysis.py`

### Experiment 3: Threshold Adjustment Experiment

**Objective:** Optimize classification threshold for precision/recall balance

**Method:**
1. Load dataset with labels (device_features_with_telemetry.csv, 789 devices)
2. Reproduce train/test split (test_size=0.3, random_state=42, stratify=y)
3. Generate probabilities for test set (237 devices, 14 critical)
4. Test thresholds: [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80]
5. Calculate metrics for each: Precision, Recall, F1, TP, FP, FN, TN
6. Plot precision-recall curve and threshold vs metrics

**Tools:** Python script `threshold_adjustment_experiment.py`

---

## ðŸ“Š Experiment 1: Critical Devices Analysis

### Overview

Three devices detected as CRITICAL in batch upload (640 devices) were analyzed for technical validation.

### Device 866207059671895 - **LEGITIMATE DETECTION** âœ…

**Probability:** 99.7% (model extremely confident)  
**Outliers Detected:** 12/30 features (40% abnormal rate)  
**Risk Assessment:** HIGH - Severe multi-dimensional degradation

**Critical Issues Identified:**

| Feature | Value | Z-Score | Severity | Interpretation |
|---------|-------|---------|----------|----------------|
| `temp_above_threshold` | 2 events | **+25.18Ïƒ** | EXTREME | Temperature exceeded safe limits twice |
| `battery_below_threshold` | 20 events | **+7.42Ïƒ** | SEVERE | Battery dropped below safe voltage 20 times |
| `temp_max` | 76Â°C | **+6.28Ïƒ** | SEVERE | Maximum temperature 76Â°C (normal ~39Â°C) |
| `battery_min` | 2.23V | **-3.94Ïƒ** | CRITICAL | Minimum battery critically low |
| `optical_below_threshold` | 6,299 events | **+4.56Ïƒ** | HIGH | Optical power frequently below threshold |
| `total_messages` | 13,652 | **+4.65Ïƒ** | HIGH | Excessive messaging (struggling connection) |
| `rsrp_std` | 9.88 | **+3.12Ïƒ** | HIGH | Signal strength highly unstable |
| `rsrp_min` | -129 dBm | **-2.08Ïƒ** | MODERATE | Very weak signal at times |

**Technical Interpretation:**

This device exhibits a **classic failing device signature** with failures across multiple subsystems:
- **Thermal Management:** Overheating (76Â°C max) with threshold breaches
- **Power System:** Battery degradation (2.23V minimum) with 20 voltage drops
- **Connectivity:** Unstable signal (rsrp_std 9.88, min -129 dBm)
- **Optical System:** 6,299 below-threshold events indicating sensor issues
- **Behavior:** 13,652 messages (vs 507 average) suggests device retrying failed connections

**Conclusion:** âœ… **LEGITIMATE** - Model correctly identified severely degraded device requiring immediate maintenance.

---

### Device 861275072514504 - **MODERATE EVIDENCE** âš ï¸

**Probability:** 82.1% (model confident)  
**Outliers Detected:** 2/30 features (6.7% abnormal rate)  
**Risk Assessment:** MODERATE - Signal instability only

**Issues Identified:**

| Feature | Value | Z-Score | Severity | Interpretation |
|---------|-------|---------|----------|----------------|
| `rsrp_std` | 13.94 | **+4.94Ïƒ** | EXTREME | Signal strength highly variable |
| `snr_std` | 5.82 | **+2.19Ïƒ** | MODERATE | Signal-to-noise ratio variable |

**Hardware Status:**
- âœ… Battery: NORMAL (all battery metrics within expected range)
- âœ… Temperature: NORMAL (all temperature metrics within expected range)
- âœ… Optical: NORMAL (all optical metrics within expected range)
- âœ… 28/30 features completely NORMAL

**Raw Message Analysis (Nov 13-14, 2025):**

From `device_861275072514504_2025-11-14.csv`:
- Last message: Recent (within 24 hours)
- Message pattern: Regular telemetry transmission
- Battery voltage: Stable operational levels
- Temperature: Within normal range

**Technical Interpretation:**

This is an **interesting edge case**:
- Device hardware appears healthy (battery, temp, optical all normal)
- BUT connectivity is highly unstable (rsrp_std z-score of +4.94Ïƒ is EXTREME)
- Signal variance (rsrp_std, snr_std) might indicate:
  1. **Environmental issues** (antenna obstruction, network interference)
  2. **Network handover problems** (device moving between cell towers)
  3. **Early stage degradation** (connectivity issues precede hardware failure)

**Hypothesis:** Model may be demonstrating **predictive capability** - flagging devices that WILL fail soon based on early warning signals (signal instability), even before hardware degradation is visible.

**Conclusion:** âš ï¸ **MODERATE** - Weak signals but high feature importance (rsrp_std is #5 most important feature). **Requires ground truth validation** to determine if this is predictive detection or environmental false positive.

**Recommendation:** Monitor device closely. If device fails within 30-60 days post-detection, this confirms model has predictive capability.

---

### Device 861275072341072 - **WEAK EVIDENCE** âŒ

**Probability:** 59.8% (borderline threshold)  
**Outliers Detected:** 1/30 features (3.3% abnormal rate)  
**Risk Assessment:** LOW - Minimal evidence of degradation

**Issue Identified:**

| Feature | Value | Z-Score | Severity | Interpretation |
|---------|-------|---------|----------|----------------|
| `rsrq_min` | -28 | **-3.65Ïƒ** | MODERATE | Poor signal quality at one historical point |

**Hardware Status:**
- âœ… 29/30 features completely NORMAL
- âœ… Battery, temperature, optical, messaging all within expected ranges

**Raw Message Analysis (Nov 13-14, 2025):**

From `device_861275072341072_2025-11-14.csv`:
- **Message pattern:** Regular hourly telemetry (msg_type=1)
- **Last 48 hours:** Consistent operational messages
- **Battery voltage:** 3.33-3.44V (healthy range)
- **Temperature:** 15-33Â°C (normal operational)
- **Signal quality:** Mix of "Sinal mÃ©dio" and "Sinal fraco" (medium/weak but functional)
- **Operational status:** Device is ACTIVE and transmitting normally

**Technical Interpretation:**

Single outlier (`rsrq_min=-28`) suggests device experienced temporary signal quality degradation at some point in its history, but:
- **Current operational state:** Device is healthy and functioning
- **Recent behavior:** Regular hourly messages indicate stable operation
- **Hardware metrics:** All within normal ranges
- **Model uncertainty:** 59.8% probability is just above 50% threshold (marginal decision)

**Hypothesis:** This detection is likely a **FALSE POSITIVE** or **OVER-SENSITIVE** threshold. The single historical signal quality event does not justify classification as critical given all other metrics are normal.

**Conclusion:** âŒ **LIKELY FALSE POSITIVE** - Device appears operational. This case supports recommendation to raise threshold from 0.50 to 0.60-0.70 to filter out borderline cases.

---

### Comparative Analysis - 3 Devices

| Device ID | Probability | Outliers | Outlier Rate | Verdict | Confidence |
|-----------|-------------|----------|--------------|---------|------------|
| 866207059671895 | 99.7% | 12/30 | 40.0% | âœ… LEGITIMATE | Very High |
| 861275072514504 | 82.1% | 2/30 | 6.7% | âš ï¸ MODERATE | Medium (pending validation) |
| 861275072341072 | 59.8% | 1/30 | 3.3% | âŒ WEAK/FP | Low |

**Average Outliers:** 5.0 features per critical device

**Key Insight:** Outlier count strongly correlates with probability and legitimacy:
- â‰¥5 outliers â†’ High confidence LEGITIMATE detection
- 2-4 outliers â†’ MODERATE evidence, needs business validation
- <2 outliers â†’ WEAK evidence, likely false positive

**Model Calibration:** Model probabilities appear well-calibrated:
- 99.7% prob â†’ 12 outliers (severe degradation)
- 82.1% prob â†’ 2 outliers (moderate signals)
- 59.8% prob â†’ 1 outlier (weak evidence)

---

## ðŸŽ¯ Experiment 2: Feature Importance Analysis

### Top 15 Most Important Features

| Rank | Feature | Importance | Contribution | Category |
|------|---------|------------|--------------|----------|
| **1** | `optical_readings` | 10.2896 | **10.29%** | Optical |
| **2** | `battery_mean` | 6.6385 | 6.64% | Battery |
| **3** | `total_messages` | 6.6188 | 6.62% | Messaging |
| **4** | `max_frame_count` | 5.2733 | 5.27% | Messaging |
| **5** | `rsrp_std` | 5.1893 | **5.19%** | Signal_RSRP |
| **6** | `optical_below_threshold` | 5.0737 | 5.07% | Optical |
| **7** | `optical_min` | 4.9334 | 4.93% | Optical |
| **8** | **`days_since_last_message`** | 4.7271 | **4.73%** | **Messaging** |
| **9** | `optical_std` | 4.0610 | 4.06% | Optical |
| **10** | `rsrp_mean` | 4.0063 | 4.01% | Signal_RSRP |
| **11** | `rsrq_mean` | 3.9629 | 3.96% | Signal_RSRQ |
| **12** | `temp_min` | 3.3837 | 3.38% | Temperature |
| **13** | `battery_std` | 3.2779 | 3.28% | Battery |
| **14** | `optical_range` | 3.1808 | 3.18% | Optical |
| **15** | `temp_max` | 2.9612 | 2.96% | Temperature |

### Feature Category Breakdown

| Category | Total Importance | Contribution | Features |
|----------|------------------|--------------|----------|
| **Optical** | 31.3939 | **31.39%** | 8 features |
| **Messaging** | 16.6192 | **16.62%** | 3 features |
| **Battery** | 13.8744 | 13.87% | 5 features |
| **Temperature** | 12.4252 | 12.43% | 6 features |
| **Signal_RSRP** | 11.8320 | 11.83% | 3 features |
| **Signal_RSRQ** | 9.1249 | 9.12% | 3 features |
| **Signal_SNR** | 4.7305 | 4.73% | 3 features |
| **Signal_RSSI** | 0.0000 | 0.00% | 0 features |

### Key Findings

#### 1. days_since_last_message Validation âœ…

**Result:** `days_since_last_message` achieved **Rank #8/30** with **4.73% contribution**

**Interpretation:**
- **TOP 10 Feature** - Contributes significantly to model predictions
- **Messaging Category:** 3rd most important within its category (after total_messages, max_frame_count)
- **FASE 2 Success:** Temporal feature addition was a VALID enhancement

**Comparison with Messaging Features:**
- `total_messages`: Rank #3 (6.62%)
- `max_frame_count`: Rank #4 (5.27%)
- `days_since_last_message`: Rank #8 (4.73%) â­

**Conclusion:** Feature temporal foi uma **ADIÃ‡ÃƒO VÃLIDA** na FASE 2. Inatividade de dispositivos Ã‰ um indicador relevante de risco de falha.

#### 2. Optical Dominance

**Optical category contributes 31.39%** of total model importance:
- `optical_readings` is #1 most important feature (10.29%)
- 4 optical features in top 10
- Explains why Device 866207059671895 was detected (6,299 optical_below_threshold events)

#### 3. Signal Variance Importance

**`rsrp_std` is Rank #5** (5.19%):
- Confirms observation from critical devices analysis
- Devices 861275072514504 and 861275072341072 both had signal variance outliers
- Model uses signal instability as strong predictor of failure

**Insight:** Signal variance (rsrp_std, snr_std) may indicate:
- Early warning of hardware degradation
- Environmental/network issues (potential false positives)
- Connectivity problems preceding device failure

---

## âš™ï¸ Experiment 3: Threshold Adjustment

### Tested Thresholds & Results

| Threshold | Precision | Recall | F1-Score | TP | FP | FN | TN | Predicted Critical |
|-----------|-----------|--------|----------|----|----|----|----|-------------------|
| **0.50** (baseline) | 83.3% | 71.4% | 0.769 | 10 | 2 | 4 | 221 | 12 |
| 0.55 | 83.3% | 71.4% | 0.769 | 10 | 2 | 4 | 221 | 12 |
| **0.60** | **90.9%** | **71.4%** | **0.800** | **10** | **1** | **4** | **222** | **11** |
| 0.65 | 90.9% | 71.4% | 0.800 | 10 | 1 | 4 | 222 | 11 |
| 0.70 | 90.9% | 71.4% | 0.800 | 10 | 1 | 4 | 222 | 11 |
| 0.75 | 90.9% | 71.4% | 0.800 | 10 | 1 | 4 | 222 | 11 |
| 0.80 | 90.0% | 64.3% | 0.750 | 9 | 1 | 5 | 222 | 10 |

**Test Set:** 237 devices (14 critical, 223 normal)

### Key Findings

#### 1. âš ï¸ Metrics Discrepancy - CRITICAL

**TRUE v2.0 FIELD-only Metrics (metadata.json - 229 test devices):**
- Precision: 57.1%
- Recall: 57.1%
- F1-Score: 0.571
- ROC-AUC: 0.9186
- Test set: 229 FIELD-only devices (14 critical, 215 normal)

**Experimental Metrics (THIS REPORT - 237 test devices MIXED):**
- Precision: **83.3%** at threshold 0.50
- Recall: **71.4%** at threshold 0.50
- Test set: 237 MIXED devices (FIELD + FACTORY)

**Why the Discrepancy:**
- âŒ Experiment used 789-device MIXED dataset (pre-Discovery 0 cleanup)
- âŒ Test set included FACTORY lifecycle data (27 devices contamination)
- âŒ Different train/test split than final v2.0 model
- âŒ Experimental metrics are INFLATED by data contamination

**CONCLUSION:** **Use 57.1% as TRUE v2.0 baseline.** Experimental metrics (83.3%/71.4%) are NOT valid for FIELD-only model.

#### 2. Optimal Threshold: 0.60-0.70

**Performance at Threshold 0.60:**
- **Precision: 90.9%** (+7.6pp vs baseline)
- **Recall: 71.4%** (maintains same detection rate)
- **F1-Score: 0.800** (BEST)
- **False Positives: 1** (50% reduction from 2 to 1)

**Business Impact:**
- **9 out of 10 alerts are true positives** (high confidence)
- **10 out of 14 critical devices detected** (acceptable miss rate for limited resources)
- **50% fewer false alarms** reduces maintenance team burden

#### 3. Threshold Stability

Thresholds 0.60-0.75 produce **identical results**:
- Same TP, FP, FN, TN counts
- Precision/Recall/F1 unchanged
- Indicates probability distribution has gap between 0.55-0.75 range

**Implication:** Threshold can be set anywhere in 0.60-0.75 range without affecting classification.

### Precision-Recall Trade-off Analysis

**Baseline (0.50):**
- Detects 12 devices as critical (2 false positives)
- Suitable if **recall is priority** (catch all failures)

**Optimal (0.60-0.70):**
- Detects 11 devices as critical (1 false positive)
- Suitable if **precision is priority** (reduce false alarms)

**Conservative (0.80):**
- Detects 10 devices as critical (1 false positive)
- **Loses 1 true positive** (9 TP vs 10 TP)
- NOT recommended - sacrifices recall without precision gain

### Experimental Insight (Not Production Recommendation)

âš ï¸ **Threshold optimization approach validated for future iterations**

**Research Finding:**
Threshold 0.60 showed 90.9% precision / 71.4% recall on experimental MIXED dataset, but these metrics DO NOT apply to v2.0 FIELD-only.

**Methodology Contribution:**
1. Demonstrated precision-recall trade-off analysis technique
2. Showed threshold adjustment can reduce false positives
3. Identified stable threshold range (0.60-0.75 produced identical results)
4. Validated approach for FASE 3 model tuning

**For v2.0 FIELD-only:** Use default threshold 0.50 with TRUE baseline 57.1% precision/recall. Threshold optimization should be re-run on clean FIELD-only test set in future iterations.

---

## ðŸŽ¯ Conclusions & Key Takeaways

### Experimental Validation Status

âš ï¸ **MODEL v2 FIELD-only - EXPERIMENTAL VALIDATION COMPLETED**

**TRUE v2.0 Performance (FIELD-only 229 test devices):**
- **Precision/Recall: 57.1%** (honest baseline)
- **ROC-AUC: 0.9186** (strong discriminative capability)
- **Status: MVP/POC validated** for research and early-stage deployment with human oversight

**Research Contributions from Experimental Analysis:**

1. **Critical Detection Methodology Validated:**
   - Device 866207059671895 (12 outliers) demonstrates multi-dimensional failure detection
   - Outlier analysis technique applicable to future validation efforts
   - Model captures temperature, battery, signal, and optical degradation patterns

2. **Temporal Feature Value Confirmed:**
   - `days_since_last_message` achieved Rank #8/30 (4.73% importance)
   - TOP 10 feature - FASE 2 enhancement was strategically valuable
   - Inactivity is a valid risk indicator for future models

3. **Threshold Optimization Technique Demonstrated:**
   - Precision-recall trade-off analysis methodology validated
   - Technique applicable to FASE 3 model tuning on clean datasets
   - Experimental finding: Threshold adjustments can significantly impact precision

### MVP Deployment Recommendations

#### 1. **Use v2.0 with Default Threshold 0.50** â­ (CURRENT STATUS)

**Current Deployment:** Streamlit app with threshold 0.50

**TRUE v2.0 Performance:**
- Precision: 57.1%
- Recall: 57.1%
- ROC-AUC: 0.9186
- **Use Case:** Early warning system with human oversight, NOT autonomous decision-making

**Experimental threshold optimization (0.60) NOT recommended** until re-validated on FIELD-only dataset.

#### 2. **Document Known Limitations** âœ… (COMPLETED)

**Create:** `docs/MODEL_V2_KNOWN_ISSUES.md`

**Content:**
- Miss rate 28.6% (4/14 critical devices not detected)
- Sensitivity to signal variance (rsrp_std, snr_std) may flag environmental issues
- Metrics discrepancy (57.1% reported vs 83.3% observed) requires investigation
- No hyperparameter tuning performed (opportunity for +10-15% recall improvement)
- Dataset size small (789 devices, 46 critical) limits generalization

#### 3. **Ground Truth Validation** (HIGH PRIORITY - IN PROGRESS)

**Action:** Validate 3 critical devices via STI Dashboard (Enzo)

**Devices to Check:**
- 866207059671895 (99.7% prob) - Expect failure/maintenance logs âœ…
- 861275072514504 (82.1% prob) - Check if failed AFTER detection âš ï¸
- 861275072341072 (59.8% prob) - If operational, confirms FP âŒ

**Purpose:** Confirm model detections correspond to real-world failures

#### 4. **Monitor Signal Variance Features** (MEDIUM PRIORITY)

**Observation:** `rsrp_std` (#5) and `snr_std` appeared in 2/3 critical detections

**Investigation Needed:**
- Are signal variance outliers caused by device hardware degradation?
- Or environmental/network issues (not device fault)?
- If latter, may need feature engineering to separate device vs environment

### Known Limitations & Issues

#### 1. Metrics Discrepancy

**Issue:** Model metadata reports 57.1% precision/recall, but experiment shows 83.3%/71.4%

**Possible Causes:**
- Different datasets (789 v1 vs 762 v2 FIELD-only)
- Feature imputation (`days_since_last_message=0` in v1 dataset)
- Different train/test split

**Action Required:** Re-run threshold experiment with EXACT v2 training/test split

#### 2. Miss Rate 28.6%

**Issue:** Model misses 4 out of 14 critical devices (28.6%)

**Impact:** Some failures may not be detected in time

**Mitigation:**
- FASE 3: Add advanced temporal features (deployment_age, activity_trends)
- Expected improvement: +20% recall
- Hyperparameter tuning: +10-15% recall

#### 3. False Positive Sensitivity

**Issue:** Device 861275072341072 (59.8% prob) likely false positive

**Root Cause:** Single historical signal quality event (rsrq_min=-28)

**Solution:** âœ… Threshold 0.60 would filter this case (prob < threshold)

#### 4. Signal Variance Ambiguity

**Issue:** `rsrp_std`, `snr_std` high importance but may flag environmental issues

**Examples:**
- Device 861275072514504: Signal unstable but hardware healthy
- Could be antenna obstruction, network handover, or early degradation

**Action Required:** Analyze correlation between signal variance and actual failures (post ground truth validation)

---

## ðŸš€ Next Steps & Roadmap

### Immediate Actions (Next 7 Days)

1. âœ… **Complete Ground Truth Validation** (CRITICAL)
   - Enzo validates 3 devices in STI Dashboard
   - Determine if detections are true positives or false positives
   - Update model confidence based on validation results

2. ðŸ”§ **Deploy Threshold 0.60** (if approved by stakeholders)
   - Update Streamlit app classification logic
   - Update documentation and Research Context
   - Test in production with batch upload (640 devices)

3. ðŸ“ **Document Known Issues**
   - Create MODEL_V2_KNOWN_ISSUES.md
   - Share with stakeholders for transparency
   - Set expectations on model limitations

### Short-Term (Next 30 Days)

4. ðŸ”¬ **Investigate Metrics Discrepancy**
   - Re-run threshold experiment with exact v2 test set
   - Confirm if baseline is 57.1% or 83.3%
   - Update model metadata if needed

5. ðŸ“Š **Sensitivity Analysis**
   - Test `days_since_last_message` impact (vary 0â†’120 days)
   - Understand temporal feature contribution
   - Document findings for FASE 3 planning

6. ðŸ” **Signal Variance Analysis**
   - Correlate `rsrp_std`, `snr_std` outliers with actual failures
   - Determine if feature flags device degradation or environmental issues
   - Consider feature engineering if needed

### Medium-Term (Next 60-90 Days - FASE 3)

7. ðŸ†• **FASE 3: Advanced Temporal Features** (if v2 validated)
   - `deployment_age` (days since first message)
   - `last_active_period` (days of recent activity)
   - `msg_last_7days`, `msg_last_30days` (activity trends)
   - `optical_delta`, `battery_delta` (change over time)
   - **Expected:** +20% recall improvement

8. âš™ï¸ **Hyperparameter Tuning**
   - GridSearchCV on CatBoost parameters
   - Test depth [4,6,8,10], iterations [100,200,300]
   - **Expected:** +10-15% recall improvement

9. ðŸ“ˆ **Continuous Monitoring**
   - Track precision/recall in production
   - Collect feedback from maintenance teams
   - Retrain model as new labeled data becomes available

---

## ðŸ“š Appendix

### A. Files Generated

**Analysis Outputs:**
- `analysis/feature_importance_complete.csv` - Full feature importance rankings
- `analysis/feature_importance_top15.png` - Top 15 features visualization
- `analysis/feature_importance_by_category.png` - Category breakdown
- `analysis/threshold_experiment_results.csv` - Threshold performance data
- `analysis/precision_recall_curve.png` - PR curve with thresholds marked
- `analysis/threshold_vs_metrics.png` - Metrics evolution by threshold

**Device Analysis:**
- `device_analysis/device_861275072514504_2025-11-14.csv` - Raw messages (MODERATE case)
- `device_analysis/device_861275072341072_2025-11-14.csv` - Raw messages (WEAK/FP case)

**Scripts:**
- `scripts/analyze_critical_devices.py` - Outlier analysis
- `scripts/feature_importance_analysis.py` - Feature importance extraction
- `scripts/threshold_adjustment_experiment.py` - Threshold optimization

### B. Model Specifications

**Model:** CatBoost v2.0 FIELD-only  
**Training Date:** November 13, 2025  
**Algorithm:** CatBoostClassifier + SMOTE 0.5  
**Features:** 30 (29 telemetry + 1 temporal)  
**Dataset:** 762 devices FIELD-only (533 train, 229 test)  
**Reported Metrics:** Recall 57.1%, Precision 57.1%, AUC 0.9186  
**Observed Metrics:** Recall 71.4%, Precision 83.3% (threshold 0.50)  

### C. References

- **Model Training:** `train_model_v2.py` (Nov 13, 2025)
- **Model Metadata:** `models/catboost_pipeline_v2_metadata.json`
- **FASE 2 Context:** `docs/PLANO_ACAO_FIX_FALSOS_POSITIVOS.md`
- **Discovery 0:** `CHANGELOG.md` - Lifecycle Contamination Fix
- **Research Context:** `pages/5_Research_Context.py` (deployed to Streamlit Cloud)

---

## âœ… Sign-Off

**Validation Status:** EXPERIMENTAL VALIDATION COMPLETE  
**Model Status:** âš ï¸ **MVP/POC VALIDATED** - Research and early-stage deployment with human oversight  
**TRUE v2.0 Baseline:** 57.1% precision/recall (metadata.json)  
**Experimental Metrics:** NOT applicable to FIELD-only v2.0 model  
**Use Case:** Early warning system, priority triaging, NOT autonomous decision-making

**Critical Note:** This report documents research methodology validation. For production deployment decisions, use TRUE baseline metrics (57.1%) from `models/catboost_pipeline_v2_metadata.json`.

**Prepared by:** Leonardo Costa  
**Original Date:** November 14, 2025  
**Revised:** November 18, 2025 (Post-Discovery 0 context update)  
**Version:** 1.1
