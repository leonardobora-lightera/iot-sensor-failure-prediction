# ğŸ›¡ï¸ CHECKLIST DE MITIGAÃ‡ÃƒO DE VIESES - Projeto IoT Sensor Failure Prediction

**VersÃ£o:** 1.0.0  
**Data:** 31 de outubro de 2025  
**Autor:** Leonardo Costa (GestÃ£o de Falhas)  
**RevisÃ£o:** Constitution v0.1.0 compliant

---

## ğŸ“‹ Ãndice

1. [Vieses de Dados (Data Biases)](#1-vieses-de-dados-data-biases)
2. [Vieses Temporais (Temporal Biases)](#2-vieses-temporais-temporal-biases)
3. [Vieses de Amostragem (Sampling Biases)](#3-vieses-de-amostragem-sampling-biases)
4. [Vieses de Modelagem (Modeling Biases)](#4-vieses-de-modelagem-modeling-biases)
5. [Vieses de ValidaÃ§Ã£o (Validation Biases)](#5-vieses-de-validaÃ§Ã£o-validation-biases)
6. [Vieses de Deployment (Production Biases)](#6-vieses-de-deployment-production-biases)
7. [Vieses Humanos (Human Biases)](#7-vieses-humanos-human-biases)

---

## 1. Vieses de Dados (Data Biases)

### 1.1 Data Leakage (Vazamento de InformaÃ§Ã£o) ğŸ”´ CRÃTICO

**O que Ã©:** InformaÃ§Ã£o do futuro ou do teste influencia o treinamento, inflando artificialmente a performance.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Split ANTES de qualquer processamento**
  - âœ… Implementado: train-test split 70/30 temporal ANTES de agregaÃ§Ãµes
  - âœ… Validado: Nenhuma feature calculada usa dados de teste
  
- [x] **FIT apenas em treino, TRANSFORM em teste**
  - âœ… Implementado: AgregaÃ§Ãµes (optical, temp, battery, signal) usam apenas `df_train.groupby()`
  - âœ… Validado: Test set aplica mesmas transformaÃ§Ãµes sem re-fit
  
- [x] **Pipeline sklearn obrigatÃ³rio**
  - â³ **PENDENTE**: Criar Pipeline em notebook 03 (feature engineering)
  - ğŸ¯ Meta: `make_pipeline(StandardScaler(), RandomForest())`
  
- [x] **Validar ausÃªncia de target leakage**
  - âœ… Implementado: Target `is_critical_target` criado APÃ“S split
  - âœ… Validado: Features nÃ£o contÃªm informaÃ§Ã£o futura de msg6_rate

**ReferÃªncia:** [Scikit-learn Data Leakage](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage)

**Status atual:** âœ… **APROVADO** (11/12 correÃ§Ãµes implementadas, Pipeline pendente)

---

### 1.2 Selection Bias (ViÃ©s de SeleÃ§Ã£o)

**O que Ã©:** Dataset nÃ£o representa a populaÃ§Ã£o real - favorece certos tipos de devices.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Verificar composiÃ§Ã£o do dataset**
  - âœ… AnÃ¡lise realizada: 789 devices, 676 com msg6 (85.7%)
  - âš ï¸ **ALERTA**: 85.7% failure rate Ã© MUITO ALTO - pode indicar prÃ©-filtragem
  - â“ **PERGUNTA P/ ENGENHARIA**: Dataset Ã© populaÃ§Ã£o completa ou apenas devices problemÃ¡ticos?

- [ ] **EstratificaÃ§Ã£o por caracterÃ­sticas**
  - â³ **TODO**: Verificar se dataset representa:
    - [ ] Diferentes versÃµes de firmware (v1.1.0_rc19, v1.2.0_rc07)
    - [ ] Diferentes operadoras (VIVO SP, RS, ParanÃ¡, Pernambuco)
    - [ ] Diferentes regiÃµes geogrÃ¡ficas
    - [ ] Diferentes idades de instalaÃ§Ã£o (devices antigos vs recentes)

- [x] **Documentar population vs sample**
  - âš ï¸ **DESCONHECIDO**: Ainda nÃ£o sabemos se dataset Ã© amostra ou populaÃ§Ã£o completa
  - ğŸ“‹ **AÃ‡ÃƒO**: Incluir em `ENGINEERING_QUESTIONS.md`

**Status atual:** âš ï¸ **ATENÃ‡ÃƒO NECESSÃRIA** (85.7% failure rate suspeito)

---

### 1.3 Measurement Bias (ViÃ©s de MediÃ§Ã£o)

**O que Ã©:** Erros sistemÃ¡ticos na coleta de telemetrias que distorcem dados.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Missing values analysis**
  - âœ… Implementado: AnÃ¡lise de % faltante por telemetria
  - âœ… Resultado: ~45% missing em optical power, temp, battery, RSSI
  - â“ **PERGUNTA**: Por que 45% faltante? Firmware antigo ou falha de sensor?

- [ ] **Validar calibraÃ§Ã£o de sensores**
  - â³ **TODO**: Verificar se thresholds sÃ£o universais ou por device
    - [ ] Optical power: -28 dBm threshold vÃ¡lido para TODOS devices?
    - [ ] Temperatura: 70Â°C threshold vÃ¡lido para TODOS ambientes?
    - [ ] Bateria: 2.5V threshold vÃ¡lido para TODOS tipos de bateria?

- [ ] **Detectar outliers instrumentais**
  - â³ **TODO**: Verificar se existem valores impossÃ­veis
    - [ ] RSSI > 0 dBm (impossÃ­vel)
    - [ ] Temperatura < -40Â°C ou > 120Â°C (fora de especificaÃ§Ã£o)
    - [ ] Battery < 0V ou > 5V (erro de leitura)

**Status atual:** âš ï¸ **ATENÃ‡ÃƒO** (45% missing values precisa investigaÃ§Ã£o)

---

### 1.4 Label Noise (RuÃ­do nos RÃ³tulos)

**O que Ã©:** Target variable incorreto ou ambÃ­guo contamina aprendizado.

#### Checklist de MitigaÃ§Ã£o:

- [x] **DefiniÃ§Ã£o clara de "falha"**
  - âœ… Definido: `is_critical_target = msg6_rate > 25%`
  - âš ï¸ **AMBIGUIDADE**: msg6 nÃ£o significa "device morto" - muitos se auto-recuperam
  - ğŸ”„ **REFINAMENTO**: Considerar multi-class (healthy, unstable, critical, failed)

- [ ] **Validar ground truth**
  - â³ **TODO**: Cross-check com devices.json (7 confirmed failures)
  - â³ **TODO**: Validar se devices "critical" realmente falharam ou sÃ£o instÃ¡veis

- [x] **AnÃ¡lise de auto-recuperaÃ§Ã£o**
  - â“ **DESCONHECIDO**: Quantos % de devices "critical" se recuperam sozinhos?
  - ğŸ“‹ **AÃ‡ÃƒO**: Adicionar anÃ¡lise temporal de recuperaÃ§Ã£o em notebook 03

**Status atual:** âš ï¸ **REFINAMENTO NECESSÃRIO** (definiÃ§Ã£o de falha ambÃ­gua)

---

## 2. Vieses Temporais (Temporal Biases)

### 2.1 Temporal Data Leakage ğŸ”´ CRÃTICO

**O que Ã©:** Usar dados futuros para prever o passado, ou misturar ordem temporal.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Train-test split temporal**
  - âœ… Implementado: 70% primeiros dias â†’ treino, 30% Ãºltimos dias â†’ teste
  - âœ… Validado: `split_date` preserva ordem cronolÃ³gica
  - âœ… CÃ³digo: `df_sorted = df.sort_values('@timestamp')`

- [x] **Forward-looking labels APENAS**
  - âœ… Implementado: Target usa msg6_rate calculado no perÃ­odo atual
  - âš ï¸ **ATENÃ‡ÃƒO**: Para prediÃ§Ã£o futura, precisamos criar `will_fail_7d_ahead`
  - â³ **TODO Notebook 03**: Criar labels forward-looking (T+7d, T+14d, T+30d)

- [x] **TimeSeriesSplit para CV**
  - âŒ **NÃƒO IMPLEMENTADO**: Atualmente usa KFold padrÃ£o (INCORRETO para temporal)
  - ğŸ”´ **CRÃTICO**: Substituir `cv=5` por `TimeSeriesSplit(n_splits=5, gap=7)`
  - ğŸ“š **ReferÃªncia**: [Scikit-learn TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)

**CÃ³digo CORRETO para CV temporal:**

```python
from sklearn.model_selection import TimeSeriesSplit

# ERRADO (atual - usa KFold random)
cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='recall')

# CERTO (temporal - treina em passado, testa em futuro)
tscv = TimeSeriesSplit(n_splits=5, gap=7, test_size=30)  # gap=7 dias entre train/test
cv_scores = cross_val_score(rf, X_train, y_train, cv=tscv, scoring='recall')
```

**Status atual:** ğŸ”´ **CRÃTICO - CORREÃ‡ÃƒO OBRIGATÃ“RIA** (TimeSeriesSplit nÃ£o usado)

---

### 2.2 Concept Drift (MudanÃ§a de Conceito)

**O que Ã©:** RelaÃ§Ã£o entre features e target muda ao longo do tempo.

#### Checklist de MitigaÃ§Ã£o:

- [ ] **AnÃ¡lise de estabilidade temporal**
  - â³ **TODO**: Calcular correlaÃ§Ã£o msg6 Ã— features por mÃªs (jan-out 2025)
  - â³ **TODO**: Verificar se RSRP correlation Ã© estÃ¡vel ou muda com tempo
  - ğŸ¯ **Meta**: Coefficient of Variation < 30% (correlaÃ§Ã£o estÃ¡vel)

- [ ] **DetecÃ§Ã£o de sazonalidade**
  - â³ **TODO**: Verificar se msg6 tem padrÃ£o semanal/mensal
  - â³ **TODO**: Testar decomposiÃ§Ã£o temporal (trend, seasonal, residual)
  - ğŸ“Š **Ferramenta**: `statsmodels.seasonal_decompose()`

- [ ] **Monitoramento de drift em produÃ§Ã£o**
  - â³ **FUTURO**: Implementar alertas se correlaÃ§Ã£o RSRP Ã— msg6 mudar >20%
  - â³ **FUTURO**: Re-treinar modelo se drift detectado

**Status atual:** â³ **TODO - Baixa prioridade** (anÃ¡lise futura)

---

### 2.3 Look-Ahead Bias (ViÃ©s de Retrospectiva)

**O que Ã©:** Usar features que nÃ£o estariam disponÃ­veis no momento da prediÃ§Ã£o real.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Validar disponibilidade de features**
  - âœ… Todas features (RSSI, battery, temp, optical) vÃªm de telemetria em tempo real
  - âœ… Nenhuma feature usa agregaÃ§Ã£o futura (rolling stats usam apenas passado)

- [ ] **Simular latÃªncia de dados**
  - â³ **TODO**: Considerar delay de telemetria (devices enviam dados 4x/dia)
  - â³ **TODO**: Features devem usar dados de T-6h (nÃ£o T-0) para ser realista

- [x] **Documentar timestamp de cada feature**
  - âœ… Implementado: `@timestamp` preservado em dataset
  - âœ… AgregaÃ§Ãµes usam `groupby('device_id')` sem leak temporal

**Status atual:** âœ… **APROVADO** (features sÃ£o causais, nÃ£o retrospectivas)

---

## 3. Vieses de Amostragem (Sampling Biases)

### 3.1 Survivorship Bias (ViÃ©s do Sobrevivente) ğŸ”´ CRÃTICO

**O que Ã©:** Dataset contÃ©m apenas devices que "sobreviveram" atÃ© coleta, excluindo os que falharam cedo.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Verificar inclusÃ£o de devices falhados**
  - âœ… Dataset contÃ©m devices com msg6_rate > 50% (provavelmente falhados)
  - âœ… 3 devices com >1000 eventos = possÃ­veis falhas definitivas
  - âš ï¸ **ALERTA**: Falta baseline de devices 100% saudÃ¡veis (apenas 113/789 = 14.3%)

- [x] **Validar perÃ­odo de observaÃ§Ã£o**
  - âœ… PerÃ­odo: Jan-Out 2025 (281 dias) - suficiente para capturar ciclo completo
  - âš ï¸ **RISCO**: Devices instalados em Out/2025 tÃªm apenas 1 mÃªs de histÃ³rico

- [ ] **AnÃ¡lise de censura (censoring)**
  - â³ **TODO**: Identificar devices removidos/substituÃ­dos antes de Out/2025
  - â³ **TODO**: Verificar se devices top offenders foram desativados (viÃ©s de censura Ã  direita)

**RecomendaÃ§Ã£o:** Incluir feature `days_active` para controlar viÃ©s de instalaÃ§Ã£o recente.

**Status atual:** âš ï¸ **ATENÃ‡ÃƒO** (apenas 14.3% devices saudÃ¡veis - baseline fraco)

---

### 3.2 Class Imbalance (Desbalanceamento de Classes)

**O que Ã©:** Classes minoritÃ¡rias (devices crÃ­ticos) sub-representadas causam viÃ©s para maioria.

#### Checklist de MitigaÃ§Ã£o:

- [x] **AnÃ¡lise de distribuiÃ§Ã£o de classes**
  - âœ… TRAIN: 45 critical (7%) vs 631 non-critical (93%) = **1:14 imbalance**
  - âœ… TEST: 42 critical (6.1%) vs 647 non-critical (93.9%) = **1:15 imbalance**
  - ğŸ”´ **SEVERO**: Imbalance >1:10 Ã© crÃ­tico para recall

- [x] **class_weight='balanced' aplicado**
  - âœ… Implementado: `RandomForestClassifier(class_weight='balanced')`
  - âš ï¸ **LIMITAÃ‡ÃƒO**: Recall ainda Ã© 30% (insuficiente)

- [ ] **SMOTE ou undersampling**
  - â³ **TODO Notebook 03**: Aplicar SMOTE para gerar synthetic minority samples
  - ğŸ¯ **Meta**: Balancear para 1:3 ou 1:2 (ao invÃ©s de 1:14)

```python
from imblearn.over_sampling import SMOTE

# ANTES: 45 critical, 631 non-critical
smote = SMOTE(sampling_strategy=0.5, random_state=42)  # 1:2 ratio
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
# DEPOIS: 315 critical, 631 non-critical
```

- [ ] **Threshold tuning**
  - â³ **TODO**: Reduzir threshold de 0.5 para 0.3 â†’ aumentar recall
  - ğŸ“Š **Ferramenta**: Precision-Recall curve para escolher threshold Ã³timo

**Status atual:** ğŸ”´ **CRÃTICO** (imbalance 1:14 causa recall 30% - inaceitÃ¡vel)

---

### 3.3 Geographic/Demographic Bias (ViÃ©s GeogrÃ¡fico)

**O que Ã©:** Dataset sobre-representa certas regiÃµes/operadoras, sub-representa outras.

#### Checklist de MitigaÃ§Ã£o:

- [ ] **EstratificaÃ§Ã£o por regiÃ£o**
  - â³ **TODO**: Analisar distribuiÃ§Ã£o de devices por estado (SP, RS, ParanÃ¡, Pernambuco)
  - â³ **TODO**: Verificar se failures concentram-se em regiÃµes especÃ­ficas (viÃ©s geogrÃ¡fico)

- [ ] **EstratificaÃ§Ã£o por operadora**
  - â³ **TODO**: Analisar VIVO vs outras operadoras
  - â“ **PERGUNTA**: Dataset contÃ©m apenas VIVO ou mÃºltiplas operadoras?

- [ ] **EstratificaÃ§Ã£o por ambiente**
  - â³ **TODO**: Indoor vs outdoor deployment (se info disponÃ­vel)
  - â³ **TODO**: Urbano vs rural (pode afetar cobertura NB-IoT e RSRP)

**Status atual:** â³ **TODO** (anÃ¡lise nÃ£o realizada ainda)

---

## 4. Vieses de Modelagem (Modeling Biases)

### 4.1 Feature Selection Bias

**O que Ã©:** Escolher features baseado em performance no teste, causando overfitting.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Feature selection ANTES de split**
  - âŒ **INCORRETO ATUAL**: Feature importance calculado em TRAIN, mas nÃ£o houve seleÃ§Ã£o prÃ©via
  - âœ… **CORRETO**: Todas features candidate foram incluÃ­das (optical, temp, battery, signal)

- [x] **Evitar p-hacking**
  - âœ… Implementado: CorrelaÃ§Ãµes calculadas ANTES de ver performance do modelo
  - âœ… Validado: NÃ£o houve iteraÃ§Ã£o manual removendo/adicionando features baseado em accuracy

- [ ] **Recursive Feature Elimination (RFE)**
  - â³ **TODO Notebook 03**: Usar RFE para seleÃ§Ã£o automatizada
  - ğŸ¯ **Meta**: Reduzir de 14 features para top 8-10

```python
from sklearn.feature_selection import RFE

rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=8, step=1)
rfe.fit(X_train, y_train)
selected_features = X_train.columns[rfe.support_]
```

**Status atual:** âœ… **APROVADO** (nenhum p-hacking detectado)

---

### 4.2 Overfitting to Noise (Sobreajuste ao RuÃ­do)

**O que Ã©:** Modelo aprende padrÃµes aleatÃ³rios especÃ­ficos do treino que nÃ£o generalizam.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Cross-validation implementado**
  - âœ… Implementado: CV=5 para feature importance
  - ğŸ”´ **INCORRETO**: Usa KFold random (nÃ£o TimeSeriesSplit temporal)
  - ğŸ¯ **CORREÃ‡ÃƒO**: Substituir por `TimeSeriesSplit(n_splits=5, gap=7)`

- [x] **RegularizaÃ§Ã£o aplicada**
  - âš ï¸ **PARCIAL**: Random Forest com `max_depth=5` limita complexidade
  - â³ **TODO**: GridSearchCV para validar se max_depth=5 Ã© Ã³timo

- [x] **ValidaÃ§Ã£o em test set NÃƒO VISTO**
  - âœ… Test set separado desde inÃ­cio (689 devices, 30% temporal)
  - âŒ **PENDENTE**: Ainda NÃƒO validamos modelo final no test set

- [ ] **AnÃ¡lise de learning curves**
  - â³ **TODO**: Plotar train vs validation score por tamanho de treino
  - ğŸ¯ **Meta**: Curvas convergem â†’ nÃ£o hÃ¡ overfitting

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    rf, X_train, y_train, cv=tscv, scoring='recall',
    train_sizes=np.linspace(0.1, 1.0, 10)
)
```

**Status atual:** âš ï¸ **ATENÃ‡ÃƒO** (CV temporal pendente, test set nÃ£o validado)

---

### 4.3 Model Selection Bias

**O que Ã©:** Escolher modelo baseado em performance no teste, invalidando generalizaÃ§Ã£o.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Baseline definido ANTES de testes**
  - âœ… Definido: Isolation Forest (recall 99.05% conforme requirements.txt)
  - âœ… Random Forest escolhido por interpretabilidade, nÃ£o apenas accuracy

- [ ] **Nested cross-validation**
  - â³ **TODO Notebook 04**: Implementar nested CV para hyperparameter tuning
  - ğŸ“š **ExplicaÃ§Ã£o**: CV interno escolhe hiperparÃ¢metros, CV externo avalia generalizaÃ§Ã£o

```python
from sklearn.model_selection import GridSearchCV, cross_val_score

# Inner CV: hyperparameter tuning
param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10]}
inner_cv = TimeSeriesSplit(n_splits=3)
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=inner_cv)

# Outer CV: generalization assessment
outer_cv = TimeSeriesSplit(n_splits=5)
nested_scores = cross_val_score(grid_search, X, y, cv=outer_cv, scoring='recall')
```

- [x] **Documentar razÃ£o de escolhas**
  - âœ… Implementado: Notebook 02 documenta por que usar Spearman (nÃ£o-linear)
  - âœ… Implementado: Documenta por que class_weight='balanced' (imbalance)

**Status atual:** âš ï¸ **ATENÃ‡ÃƒO** (nested CV pendente)

---

### 4.4 SNR Contradiction (Problema EspecÃ­fico do Projeto)

**O que Ã©:** SNR tem feature importance #1 (30.7%) mas correlaÃ§Ã£o Spearman râ‰ˆ0 (nÃ£o significativa).

#### Checklist de MitigaÃ§Ã£o:

- [ ] **Investigar interaÃ§Ãµes nÃ£o-lineares**
  - â³ **TODO Notebook 03**: Gerar SHAP values para entender contribuiÃ§Ã£o SNR
  - â³ **TODO**: Testar se SNR Ã— Battery ou SNR Ã— RSRP tem interaÃ§Ã£o
  - ğŸ“Š **Ferramenta**: `shap.TreeExplainer(rf)`

```python
import shap

explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_train)
shap.summary_plot(shap_values[1], X_train, feature_names=feature_cols)
```

- [ ] **Partial Dependence Plots**
  - â³ **TODO**: Plotar PDP para SNR vs msg6_rate
  - ğŸ¯ **Meta**: Verificar se relaÃ§Ã£o Ã© U-shaped ou threshold-based

- [ ] **DecisÃ£o: manter ou remover SNR**
  - â³ **BLOQUEADO**: Aguardando investigaÃ§Ã£o SHAP
  - ğŸ”€ **OpÃ§Ãµes**:
    1. Manter SNR se SHAP mostrar interaÃ§Ã£o vÃ¡lida
    2. Remover SNR se importance for spurious correlation

**Status atual:** ğŸ”´ **BLOQUEADOR** (contradiÃ§Ã£o precisa investigaÃ§Ã£o urgente)

---

## 5. Vieses de ValidaÃ§Ã£o (Validation Biases)

### 5.1 Metric Gaming (OtimizaÃ§Ã£o de MÃ©trica Errada)

**O que Ã©:** Maximizar mÃ©trica que nÃ£o reflete objetivo real do negÃ³cio.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Definir mÃ©trica de negÃ³cio PRIMEIRO**
  - âœ… Definido: **RECALL >70%** (capturar falhas reais)
  - âœ… Justificativa: Falso negativo (device falha nÃ£o detectado) Ã© mais custoso que falso positivo

- [x] **Validar alignment com objetivo**
  - âš ï¸ **DESALINHADO**: Recall atual 30% << meta 70%
  - âœ… Accuracy NÃƒO Ã© mÃ©trica principal (seria 93% trivialmente predizendo "healthy")

- [ ] **Threshold tuning para recall**
  - â³ **TODO**: Ajustar threshold de decisÃ£o para maximizar recall
  - ğŸ¯ **Meta**: Encontrar threshold onde recall â‰¥70% e precision â‰¥40%

```python
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)
# Escolher threshold onde recall >= 0.7
optimal_threshold = thresholds[np.where(recalls >= 0.7)[0][0]]
```

**Status atual:** âš ï¸ **DESALINHADO** (recall 30% vs meta 70%)

---

### 5.2 Multiple Testing Problem

**O que Ã©:** Testar muitas hipÃ³teses aumenta chance de encontrar correlaÃ§Ã£o espÃºria (falso positivo).

#### Checklist de MitigaÃ§Ã£o:

- [x] **Bonferroni correction**
  - â³ **TODO**: Aplicar correÃ§Ã£o para mÃºltiplas comparaÃ§Ãµes
  - ğŸ¯ **Exemplo**: Se testamos 14 features, p-value threshold = 0.05/14 = 0.0036

- [x] **Pre-register hypotheses**
  - âœ… Implementado: HipÃ³teses documentadas ANTES de testes (Notebook 02 header)
  - âœ… Exemplo: "RSRP baixo â†’ mais msg6" (predito pela fÃ­sica, nÃ£o exploraÃ§Ã£o)

- [ ] **Holdout test set final**
  - âœ… Test set separado desde inÃ­cio
  - âŒ **PENDENTE**: NÃƒO testamos ainda (serÃ¡ validaÃ§Ã£o ÃšNICA e final)

**Status atual:** âœ… **BOM** (hipÃ³teses pre-registered, correÃ§Ã£o Bonferroni pendente)

---

### 5.3 Train-Test Contamination

**O que Ã©:** InformaÃ§Ã£o do teste vaza para treino atravÃ©s de decisÃµes humanas.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Blind analysis**
  - âœ… Implementado: Test set processado mas NÃƒO validado ainda
  - âœ… DecisÃµes de features foram baseadas apenas em TRAIN correlations

- [x] **Documentar decisÃµes**
  - âœ… Implementado: Todas decisÃµes registradas em notebooks com justificativas
  - âœ… Exemplo: SNR removido por correlaÃ§Ã£o zero (decisÃ£o baseada em treino)

- [ ] **Test set Ãºnico uso**
  - â³ **COMPROMISSO**: Usar test set UMA VEZ APENAS para validaÃ§Ã£o final
  - ğŸš« **PROIBIDO**: Iterar hiperparÃ¢metros baseado em test performance

**Status atual:** âœ… **APROVADO** (test set preservado como "virgin data")

---

## 6. Vieses de Deployment (Production Biases)

### 6.1 Train-Serve Skew

**O que Ã©:** DiferenÃ§as entre ambiente de treino e produÃ§Ã£o causam degradaÃ§Ã£o.

#### Checklist de MitigaÃ§Ã£o:

- [ ] **Validar latÃªncia de telemetria**
  - â³ **TODO**: Confirmar se devices enviam dados em tempo real ou com delay
  - â“ **PERGUNTA**: Qual Ã© delay tÃ­pico entre mediÃ§Ã£o e recebimento no servidor?

- [ ] **Validar disponibilidade de features**
  - â³ **TODO**: Confirmar que RSSI, battery, temp, optical estarÃ£o SEMPRE disponÃ­veis em produÃ§Ã£o
  - âš ï¸ **RISCO**: 45% missing values em treino â†’ produÃ§Ã£o pode ter ainda mais falta

- [ ] **Simular production environment**
  - â³ **FUTURO**: Testar modelo em ambiente staging com dados reais antes de deploy

**Status atual:** â³ **TODO - MÃ©dia prioridade**

---

### 6.2 Feedback Loop Bias

**O que Ã©:** PrediÃ§Ãµes do modelo influenciam dados futuros, criando auto-reforÃ§o.

#### Checklist de MitigaÃ§Ã£o:

- [ ] **Monitorar distribuiÃ§Ã£o de features**
  - â³ **FUTURO**: Alertar se RSRP distribution muda >20% em produÃ§Ã£o
  - ğŸ¯ **Ferramenta**: KS-test para detectar drift

- [ ] **Randomized intervention**
  - â³ **FUTURO**: Intervir aleatoriamente em 10% dos devices preditos como "healthy"
  - ğŸ¯ **Objetivo**: Validar se prediÃ§Ãµes "safe" realmente sÃ£o safe

- [ ] **Counterfactual logging**
  - â³ **FUTURO**: Registrar o que TERIA acontecido sem intervenÃ§Ã£o

**Status atual:** â³ **FUTURO - PÃ³s-deployment**

---

## 7. Vieses Humanos (Human Biases)

### 7.1 Confirmation Bias (ViÃ©s de ConfirmaÃ§Ã£o)

**O que Ã©:** Analista busca evidÃªncias que confirmam hipÃ³tese inicial, ignorando contra-evidÃªncias.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Pre-register hypotheses**
  - âœ… Implementado: HipÃ³teses documentadas no header do notebook ANTES de anÃ¡lise
  - âœ… Exemplo: "Optical power degradaÃ§Ã£o â†’ falha" (predito, nÃ£o descoberto)

- [x] **Documentar surpresas**
  - âœ… Implementado: SNR contradiÃ§Ã£o documentada como PROBLEMA, nÃ£o ignorada
  - âœ… Implementado: RSSI correlation -0.19 aceita mesmo sendo "fraca"

- [ ] **Peer review obrigatÃ³rio**
  - âœ… **REALIZADO**: EstagiÃ¡rio colega identificou survivorship bias (validaÃ§Ã£o externa!)
  - â³ **TODO**: Solicitar revisÃ£o de Engenharia/Enzo antes de deploy

**Status atual:** âœ… **BOM** (peer review funcionou - survivorship bias detectado)

---

### 7.2 Sunk Cost Fallacy (FalÃ¡cia do Custo Afundado)

**O que Ã©:** Continuar com abordagem ruim porque "jÃ¡ investimos muito tempo".

#### Checklist de Mitigation:

- [x] **Decision gates definidos**
  - âœ… Implementado: Gate #1 "Se recall <70% â†’ PARAR" (Constitution Principle)
  - âš ï¸ **STATUS**: Recall atual 30% â†’ tecnicamente deveria PARAR
  - ğŸ”€ **DECISÃƒO**: Continuar mas reconhecer que abordagem atual FALHOU

- [x] **Kill switches**
  - âœ… Definido: Se test set recall <50% â†’ DESCARTAR modelo, nÃ£o deploy
  - âœ… Definido: Se precision <40% â†’ alarmes falsos inaceitÃ¡veis

- [ ] **Alternativas documentadas**
  - â³ **TODO**: Documentar Plano B se Random Forest falhar
  - ğŸ”€ **OpÃ§Ãµes**: XGBoost, Isolation Forest (original), LSTM temporal

**Status atual:** âš ï¸ **ATENÃ‡ÃƒO** (recall 30% indica problema, mas ainda nÃ£o acionamos kill switch)

---

### 7.3 Publication Bias (ViÃ©s de PublicaÃ§Ã£o)

**O que Ã©:** Reportar apenas resultados positivos, esconder experimentos falhados.

#### Checklist de MitigaÃ§Ã£o:

- [x] **Documentar falhas**
  - âœ… Implementado: Notebook 02 documenta SNR contradiÃ§Ã£o (nÃ£o esconde)
  - âœ… Implementado: CHECKPOINT documenta recall 30% como PROBLEMA

- [x] **Version control**
  - âœ… Implementado: Git commit 4c46ca9 preserva histÃ³rico completo
  - âœ… Implementado: Notebooks antigos arquivados em `outdated-notebooks/`

- [x] **TransparÃªncia com stakeholders**
  - âœ… Implementado: P.O. informado sobre survivorship bias
  - âœ… Implementado: Mariana validou "continuar mesmo sem resultados promissores"

**Status atual:** âœ… **EXCELENTE** (transparÃªncia total, falhas documentadas)

---

## ğŸ“Š SCORECARD DE MITIGAÃ‡ÃƒO

### Status por Categoria

| Categoria | Status | CrÃ­ticos | Pendentes | Aprovados |
|-----------|--------|----------|-----------|-----------|
| **1. Data Biases** | âš ï¸ | 0 | 3 | 8 |
| **2. Temporal Biases** | ğŸ”´ | 1 | 4 | 2 |
| **3. Sampling Biases** | ğŸ”´ | 1 | 5 | 2 |
| **4. Modeling Biases** | âš ï¸ | 1 | 7 | 3 |
| **5. Validation Biases** | âœ… | 0 | 2 | 6 |
| **6. Deployment Biases** | â³ | 0 | 6 | 0 |
| **7. Human Biases** | âœ… | 0 | 1 | 7 |
| **TOTAL** | âš ï¸ | **3** | **28** | **28** |

### Resumo Executivo

**âœ… APROVADO (28 itens):**
- Data leakage prevention implementado corretamente
- Features causais (nÃ£o retrospectivas)
- Test set preservado como virgin data
- TransparÃªncia e documentaÃ§Ã£o de falhas

**ğŸ”´ CRÃTICO (3 itens):**
1. **TimeSeriesSplit NÃƒO usado** â†’ KFold random invalida CV temporal
2. **Class imbalance 1:14** â†’ Recall 30% inaceitÃ¡vel (meta: 70%)
3. **SNR contradiÃ§Ã£o** â†’ Feature importance #1 mas correlaÃ§Ã£o zero

**âš ï¸ ATENÃ‡ÃƒO (28 itens pendentes):**
- 85.7% failure rate no dataset (suspeito de seleÃ§Ã£o)
- 45% missing values em telemetrias (precisa investigaÃ§Ã£o)
- Nested CV para hyperparameters nÃ£o implementado
- Production deployment planning pendente

---

## ğŸ¯ AÃ‡Ã•ES PRIORITÃRIAS (Segunda-feira)

### ğŸ”´ URGENTE (Bloqueadores)

1. **Substituir KFold por TimeSeriesSplit**
   ```python
   # ANTES (ERRADO)
   cv_scores = cross_val_score(rf, X_train, y_train, cv=5)
   
   # DEPOIS (CORRETO)
   from sklearn.model_selection import TimeSeriesSplit
   tscv = TimeSeriesSplit(n_splits=5, gap=7, test_size=30)
   cv_scores = cross_val_score(rf, X_train, y_train, cv=tscv)
   ```

2. **Aplicar SMOTE para class imbalance**
   ```python
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(sampling_strategy=0.5, random_state=42)
   X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
   ```

3. **Investigar SNR com SHAP values**
   ```python
   import shap
   explainer = shap.TreeExplainer(rf)
   shap_values = explainer.shap_values(X_train)
   shap.summary_plot(shap_values[1], X_train)
   ```

### âš ï¸ IMPORTANTE (Alta prioridade)

4. Validar modelo em test set (689 devices)
5. Threshold tuning para maximizar recall
6. Questionar Engenharia sobre 85.7% failure rate e 45% missing values

### â³ PLANEJADO (MÃ©dia prioridade)

7. Nested CV para hyperparameter tuning
8. AnÃ¡lise de estabilidade temporal (concept drift)
9. Feature engineering temporal (rolling stats)
10. Pipeline sklearn completo

---

## ğŸ“š ReferÃªncias

1. [Scikit-learn Common Pitfalls](https://scikit-learn.org/stable/common_pitfalls.html)
2. [TimeSeriesSplit Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)
3. [Imbalanced-learn SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)
4. [SHAP Values for XAI](https://github.com/slundberg/shap)
5. Constitution v0.1.0 - Ground-Truth First Principle

---

**Documento vivo - Atualizar apÃ³s cada milestone**  
**Ãšltima revisÃ£o:** 31/out/2025  
**PrÃ³xima revisÃ£o:** ApÃ³s validaÃ§Ã£o test set (Segunda-feira)

---

*"In God we trust, all others must bring data... and mitigate biases."*  
â€” Adaptado de W. Edwards Deming
