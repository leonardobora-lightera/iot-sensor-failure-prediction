# üõ°Ô∏è CHECKLIST: Mitiga√ß√£o de Vieses em An√°lises de Observabilidade IoT

**Projeto:** Observabilidade e Monitoramento NB-IoT  
**Contexto:** An√°lises DESCRITIVAS e INDICATIVAS (n√£o preditivas)  
**Data:** 03 de novembro de 2025  

---

## üìã √çNDICE DE VIESES

1. [Vi√©s de Sele√ß√£o](#1-vi%C3%A9s-de-sele%C3%A7%C3%A3o-selection-bias)
2. [Vi√©s de Sobreviv√™ncia](#2-vi%C3%A9s-de-sobreviv%C3%AAncia-survivorship-bias)
3. [Vi√©s de Confirma√ß√£o](#3-vi%C3%A9s-de-confirma%C3%A7%C3%A3o-confirmation-bias)
4. [Vi√©s Temporal](#4-vi%C3%A9s-temporal-temporal-bias)
5. [Vi√©s de Agrega√ß√£o](#5-vi%C3%A9s-de-agrega%C3%A7%C3%A3o-aggregation-bias)
6. [Vi√©s de Amostragem](#6-vi%C3%A9s-de-amostragem-sampling-bias)
7. [Vi√©s de Relat√≥rio](#7-vi%C3%A9s-de-relat%C3%B3rio-reporting-bias)
8. [Vi√©s de Interpreta√ß√£o](#8-vi%C3%A9s-de-interpreta%C3%A7%C3%A3o-interpretation-bias)
9. [Vi√©s de Medi√ß√£o](#9-vi%C3%A9s-de-medi%C3%A7%C3%A3o-measurement-bias)
10. [Vi√©s Operacional](#10-vi%C3%A9s-operacional-operational-bias)

---

## 1. Vi√©s de Sele√ß√£o (Selection Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**ALTO** - Dataset cont√©m apenas devices que **reportaram logs** ao AWS

### üìå Manifesta√ß√µes Poss√≠veis

#### 1.1 Devices Silenciosos

**Problema:** Devices que falharam **completamente** n√£o aparecem nos logs  
**Exemplo:** Device com bateria totalmente esgotada n√£o envia msg_type 6

**ATUALIZA√á√ÉO 13/Nov/2025 - VALIDADO EM PRODU√á√ÉO:**

**Problema Espec√≠fico Identificado:** Devices **INATIVOS** ap√≥s testes de laborat√≥rio sendo classificados como cr√≠ticos

**Case Study:** Device 861275072515287
- **Probabilidade predita:** 97.5% (HIGH RISK)
- **Realidade:** FALSO POSITIVO por inatividade p√≥s-laborat√≥rio
- **Evid√™ncias:**
  - √öltima comunica√ß√£o: 31/10/2025 17:35 (12 dias de inatividade)
  - Telemetrias FIELD saud√°veis: optical -12.30 dBm, temp 27¬∞C, battery 3.40V
  - Distribui√ß√£o MODE: 465 FIELD + 38 FACTORY + 174 NaN
  - Shutdown planejado: 3x msg_type 43 (heartbeat sem telemetrias)

**Problema Raiz:** Modelo agrega features de TODO o lifecycle (FACTORY + FIELD) sem contexto temporal

**Checklist de Mitiga√ß√£o ATUALIZADO:**
- [x] ‚úÖ **IMPLEMENTADO:** Filtrar MODE='FIELD' em transform_aws_payload.py
- [x] ‚úÖ **IMPLEMENTADO:** Adicionar feature days_since_last_message
- [ ] Retreinar modelo com features production-only (FASE 2 - Esta Semana)
- [ ] Classificar devices: "INACTIVE_NEEDS_INVESTIGATION" (>7 dias) vs "CRITICAL_ACTIVE" (<7 dias)
- [ ] Adicionar warning no dashboard: "X devices inativos >7 dias - n√£o s√£o falhas ativas"
- [ ] Documentar no dashboard: "An√°lise cobre apenas devices ATIVOS em produ√ß√£o (MODE=FIELD)"
- [ ] Criar m√©trica: "% devices ativos nos √∫ltimos 7/30 dias"
- [ ] Implementar features temporais completas (FASE 3 - 2 semanas)

**Mitiga√ß√£o Definitiva (ROADMAP):**
1. **QUICK WIN (HOJE):** Filtro MODE='FIELD' + days_since_last_message
2. **M√âDIO PRAZO (ESTA SEMANA):** Retreinamento com features production-only
3. **LONGO PRAZO (2 SEMANAS):** Features temporais completas (FEATURE_ENGINEERING_TEMPORAL.md)

**Refer√™ncias:**
- `docs/TEMPORAL_LIMITATIONS.md` - Documenta√ß√£o completa das limita√ß√µes
- `docs/FEATURE_ENGINEERING_TEMPORAL.md` - Roadmap de features temporais
- `device_861275072515287_2025-11-13.csv` - Case study completo
- `analyze_device_861275072515287.py` - Script de an√°lise temporal

#### 1.2 Firmware Antigo

**Problema:** Devices com firmware antigo podem n√£o reportar certas telemetrias (optical_power, RSSI)  
**Exemplo:** 45% missing values em telemetrias pode indicar vers√£o antiga

**Checklist de Mitiga√ß√£o:**
- [ ] Agrupar devices por fw_app_version e calcular % missing telemetry
- [ ] Documentar correla√ß√£o: "Firmware <v1.1.0 n√£o reporta RSSI"
- [ ] Adicionar filtro no dashboard: "Apenas devices com firmware v1.1.0+"
- [ ] Criar se√ß√£o "Cobertura de Dados" mostrando % devices com cada telemetria
- [ ] Alertar stakeholders sobre limita√ß√£o de an√°lise em devices antigos

#### 1.3 Carrier-Specific Bias

**Problema:** VIVO pode ter cobertura NB-IoT diferente de outras carriers  
**Exemplo:** Padr√µes observados em VIVO podem n√£o se aplicar a TIM/Claro

**Checklist de Mitiga√ß√£o:**
- [ ] Segmentar an√°lises por carrier (VIVO, TIM, Claro, etc)
- [ ] Calcular distribui√ß√£o de devices por carrier
- [ ] Documentar: "An√°lise baseada principalmente em dados VIVO (X%)"
- [ ] Evitar generalizar padr√µes VIVO para outras carriers sem valida√ß√£o
- [ ] Criar an√°lises comparativas quando houver dados suficientes de m√∫ltiplas carriers

---

## 2. Vi√©s de Sobreviv√™ncia (Survivorship Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**M√âDIO** - J√° validado em 30/Out que devices se recuperam ap√≥s msg_type 6

### üìå Manifesta√ß√µes Poss√≠veis

#### 2.1 Devices "Condenados"

**Problema:** Assumir que device com msg_type 6 est√° permanentemente falhado  
**Exemplo:** "Device X tem 100 msg_type 6 ‚Üí Device X est√° quebrado" (FALSO)

**Checklist de Mitiga√ß√£o:**
- [x] ‚úÖ **J√Å VALIDADO:** Devices se recuperam (reuni√£o 03/Nov confirmou)
- [ ] Calcular taxa de auto-recupera√ß√£o: "X% devices voltam a funcionar ap√≥s msg_type 6"
- [ ] Criar m√©trica "Tempo m√©dio de recupera√ß√£o" por tipo de erro
- [ ] Documentar no dashboard: "msg_type 6 N√ÉO significa falha permanente"
- [ ] Adicionar visualiza√ß√£o: "Timeline de falha ‚Üí recupera√ß√£o ‚Üí nova falha"

#### 2.2 Devices Substitu√≠dos

**Problema:** Devices que pararam de reportar podem ter sido substitu√≠dos (n√£o falharam)  
**Exemplo:** Device sem logs h√° 60 dias pode ter sido trocado em manuten√ß√£o programada

**Checklist de Mitiga√ß√£o:**
- [ ] Cruzar dados de logs com dados de manuten√ß√£o/substitui√ß√£o (se dispon√≠vel)
- [ ] Criar m√©trica: "√öltimo log h√° X dias" vs "Device marcado como substitu√≠do"
- [ ] Evitar concluir "device falhou" sem confirmar com dados de campo
- [ ] Adicionar filtro: "Excluir devices substitu√≠dos das an√°lises"

#### 2.3 Vi√©s de "Sobreviventes Saud√°veis"

**Problema:** Devices que **nunca** falharam podem ter caracter√≠sticas diferentes (hardware, instala√ß√£o, ambiente)  
**Exemplo:** Devices indoor vs outdoor podem ter taxas de falha muito diferentes

**Checklist de Mitiga√ß√£o:**
- [ ] Incluir devices com **zero** msg_type 6 nas an√°lises descritivas
- [ ] Criar grupo de controle: "Devices saud√°veis (0 msg_type 6 em 6 meses)"
- [ ] Comparar caracter√≠sticas: Firmware, regi√£o, carrier entre saud√°veis vs falhadores
- [ ] Documentar limita√ß√£o: "N√£o sabemos se devices saud√°veis t√™m melhor hardware ou melhor ambiente"

---

## 3. Vi√©s de Confirma√ß√£o (Confirmation Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**ALTO** - Risco de buscar padr√µes que confirmem hip√≥teses pr√©-existentes

### üìå Manifesta√ß√µes Poss√≠veis

#### 3.1 "Temperatura Causa Falhas"

**Problema:** Enzo mencionou temperatura ‚Üí Risco de for√ßar correla√ß√£o temperatura √ó msg_type 6  
**Exemplo:** Encontrar correla√ß√£o fraca (r=0.1) e interpretar como "confirmado"

**Checklist de Mitiga√ß√£o:**
- [ ] Definir threshold de correla√ß√£o **antes** de analisar (ex: |r| > 0.3 para "relevante")
- [ ] Calcular p-value e exigir p < 0.01 para "estatisticamente significativo"
- [ ] Testar hip√≥tese OPOSTA: "Temperatura N√ÉO correlaciona com falhas"
- [ ] Documentar correla√ß√µes **fracas** honestamente: "r=0.15 sugere correla√ß√£o fraca"
- [ ] Evitar cherry-picking: Reportar **todas** correla√ß√µes testadas, n√£o s√≥ as significativas

#### 3.2 "RSSI Explica Tudo"

**Problema:** Notebook 02 mostrou RSRP como top correla√ß√£o ‚Üí Risco de focar excessivamente em sinal  
**Exemplo:** Ignorar outros fatores (bateria, erro de firmware) ao diagnosticar falha

**Checklist de Mitiga√ß√£o:**
- [ ] Criar an√°lise multivariada: "Falhas com RSSI ALTO e bateria BAIXA"
- [ ] Documentar: "RSSI explica X% da vari√¢ncia, Y% permanece inexplicado"
- [ ] Adicionar se√ß√£o dashboard: "Falhas SEM correla√ß√£o com RSSI (Z%)"
- [ ] Evitar t√≠tulo simplista: "RSSI causa falhas" ‚Üí Usar: "RSSI correlaciona com falhas"

#### 3.3 An√°lise Seletiva de Devices

**Problema:** Focar apenas em "serial offenders" (top 5 devices com mais msg_type 6)  
**Exemplo:** Ignorar padr√£o emergente em devices com 10-50 msg_type 6

**Checklist de Mitiga√ß√£o:**
- [ ] Analisar **toda distribui√ß√£o**: Baixa (1-10), Moderada (11-50), Alta (51-100), Cr√≠tica (>100)
- [ ] Criar visualiza√ß√µes para cada segmento, n√£o s√≥ extremos
- [ ] Documentar: "Padr√£o X aparece em 80% dos devices, n√£o s√≥ top 5"
- [ ] Evitar generalizar padr√µes de outliers para popula√ß√£o geral

---

## 4. Vi√©s Temporal (Temporal Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**ALTO** - Dataset cobre Jan-Out 2025, padr√µes podem mudar ao longo do tempo

### üìå Manifesta√ß√µes Poss√≠veis

#### 4.1 Sazonalidade N√£o Identificada

**Problema:** Padr√µes de Jan podem n√£o se aplicar a Out  
**Exemplo:** Temperatura externa em Jan (ver√£o BR) vs Jul (inverno BR)

**Checklist de Mitiga√ß√£o:**
- [ ] Calcular correla√ß√µes **por m√™s** e verificar estabilidade temporal
- [ ] Criar visualiza√ß√£o: "Correla√ß√£o temperatura √ó msg6_rate por m√™s"
- [ ] Testar sazonalidade com decomposi√ß√£o de s√©ries temporais (STL decomposition)
- [ ] Documentar: "Correla√ß√£o v√°lida para per√≠odo Jan-Out 2025"
- [ ] Adicionar warning se padr√£o muda >30% entre meses

#### 4.2 Efeito de Upgrades de Firmware

**Problema:** Upgrade de firmware pode reduzir msg_type 6 ‚Üí Correla√ß√£o esp√∫ria  
**Exemplo:** Redu√ß√£o de falhas em Ago pode ser devido a firmware v1.2.0, n√£o sazonalidade

**Checklist de Mitiga√ß√£o:**
- [ ] Mapear datas de upgrades de firmware (v1.0.1 ‚Üí v1.1.0 ‚Üí v1.2.0)
- [ ] Criar marcadores no gr√°fico temporal: "Upgrade v1.1.0 em 15/Mar/2025"
- [ ] Segmentar an√°lise: "Antes de upgrade X" vs "Depois de upgrade X"
- [ ] Documentar: "Redu√ß√£o de 20% em msg6_rate ap√≥s upgrade v1.2.0"
- [ ] Evitar atribuir redu√ß√£o a fatores ambientais se coincide com upgrade

#### 4.3 Degrada√ß√£o Progressiva vs Eventos Pontuais

**Problema:** Confundir falha progressiva (bateria degrada lentamente) com evento pontual (queda de energia)  
**Exemplo:** "Bateria causa falhas" quando na verdade foi blackout regional

**Checklist de Mitiga√ß√£o:**
- [ ] Calcular rolling statistics (7d, 30d) para identificar tend√™ncias vs picos
- [ ] Criar visualiza√ß√£o: "Falhas graduais (aumento constante) vs Falhas em rajada"
- [ ] Cruzar dados de msg_type 6 com eventos conhecidos (manuten√ß√µes, blackouts)
- [ ] Adicionar filtro: "Excluir dias com eventos extraordin√°rios"
- [ ] Documentar eventos: "Pico de falhas em 10/Mai/2025 coincide com manuten√ß√£o programada"

---

## 5. Vi√©s de Agrega√ß√£o (Aggregation Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**M√âDIO** - An√°lises agregam dados por device_id, regi√£o, carrier, etc

### üìå Manifesta√ß√µes Poss√≠veis

#### 5.1 Simpson's Paradox

**Problema:** Correla√ß√£o positiva em n√≠vel agregado, negativa em n√≠vel individual  
**Exemplo:** "Temperatura alta ‚Üí mais falhas" agregado, mas "Temperatura alta ‚Üí menos falhas" em SP

**Checklist de Mitiga√ß√£o:**
- [ ] Sempre calcular correla√ß√µes em **m√∫ltiplos n√≠veis**: global, por regi√£o, por carrier
- [ ] Criar visualiza√ß√£o: "Correla√ß√£o por regi√£o" (scatter plot facetado)
- [ ] Documentar discrep√¢ncias: "Global r=0.3, mas SP r=-0.2, PE r=0.5"
- [ ] Evitar conclus√µes globais sem validar em subgrupos
- [ ] Adicionar warning: "Padr√£o varia significativamente entre regi√µes"

#### 5.2 Heterogeneidade de Devices

**Problema:** Devices com hardware diferente (vers√µes antigas vs novas) misturados na mesma an√°lise  
**Exemplo:** Device 2020 vs Device 2024 t√™m caracter√≠sticas completamente diferentes

**Checklist de Mitiga√ß√£o:**
- [ ] Segmentar por sn_fkw (serial number) ou fw_app_version
- [ ] Criar grupos: "Devices antigos (<v1.1.0)" vs "Devices novos (>=v1.1.0)"
- [ ] Calcular estat√≠sticas separadamente para cada grupo
- [ ] Documentar: "An√°lise cobre X% devices novos, Y% devices antigos"
- [ ] Evitar comparar m√©dias globais sem considerar heterogeneidade

#### 5.3 Granularidade Temporal

**Problema:** Agregar por dia pode esconder padr√µes hor√°rios  
**Exemplo:** "Sem padr√£o di√°rio" quando na verdade falhas ocorrem √†s 3h AM

**Checklist de Mitiga√ß√£o:**
- [ ] Analisar **m√∫ltiplas granularidades**: hor√°ria, di√°ria, semanal, mensal
- [ ] Criar heatmap hora √ó dia da semana
- [ ] Testar autocorrela√ß√£o temporal (lag=1h, 24h, 7d)
- [ ] Documentar: "Padr√£o hor√°rio: pico √†s 3-4h AM (hor√°rio GMT-3)"
- [ ] Evitar agregar dados sem verificar se padr√£o fino se perde

---

## 6. Vi√©s de Amostragem (Sampling Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**M√âDIO** - Dataset pode n√£o representar popula√ß√£o total de devices

### üìå Manifesta√ß√µes Poss√≠veis

#### 6.1 Dataset Est√°tico vs Frota Din√¢mica

**Problema:** Dataset cobre Jan-Out 2025, mas devices foram instalados em momentos diferentes  
**Exemplo:** Device instalado em Set/2025 tem apenas 1 m√™s de dados (vs 10 meses para devices de Jan)

**Checklist de Mitiga√ß√£o:**
- [ ] Calcular "Dias de opera√ß√£o" por device (timestamp √∫ltimo log - timestamp primeiro log)
- [ ] Criar histograma: "Distribui√ß√£o de dias de opera√ß√£o"
- [ ] Filtrar an√°lises: "Apenas devices com >90 dias de opera√ß√£o"
- [ ] Documentar: "X% devices com <30 dias de dados exclu√≠dos de an√°lise temporal"
- [ ] Evitar comparar devices novos com devices antigos sem normalizar tempo

#### 6.2 Vi√©s Geogr√°fico

**Problema:** Dataset pode ter concentra√ß√£o regional (ex: 80% devices em SP)  
**Exemplo:** Padr√µes observados refletem SP, n√£o Brasil

**Checklist de Mitiga√ß√£o:**
- [ ] Calcular distribui√ß√£o geogr√°fica: "SP: X%, PE: Y%, RS: Z%"
- [ ] Criar mapa de calor: "Devices por estado"
- [ ] Documentar limita√ß√£o: "An√°lise representa principalmente regi√£o Sudeste"
- [ ] Evitar generalizar: "Padr√£o brasileiro" quando √© "Padr√£o SP"
- [ ] Adicionar filtro: "An√°lise restrita a regi√£o X"

#### 6.3 Vi√©s de Carrier (NB-IoT)

**Problema:** VIVO √© 90% dos devices ‚Üí Padr√µes s√£o espec√≠ficos de VIVO, n√£o NB-IoT gen√©rico  
**Exemplo:** Cobertura VIVO em SP √© diferente de TIM em PE

**Checklist de Mitiga√ß√£o:**
- [ ] Calcular distribui√ß√£o por carrier: "VIVO: X%, TIM: Y%, Claro: Z%"
- [ ] Documentar: "An√°lise baseada em VIVO (X% do dataset)"
- [ ] Evitar t√≠tulo: "Padr√µes NB-IoT" ‚Üí Usar: "Padr√µes NB-IoT (rede VIVO)"
- [ ] Adicionar disclaimer: "Conclus√µes podem n√£o se aplicar a outras carriers"
- [ ] Validar padr√µes em m√∫ltiplas carriers quando poss√≠vel

---

## 7. Vi√©s de Relat√≥rio (Reporting Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**ALTO** - Dashboard ser√° usado por stakeholders para tomar decis√µes

### üìå Manifesta√ß√µes Poss√≠veis

#### 7.1 Cherry-Picking de Insights

**Problema:** Apresentar apenas correla√ß√µes significativas, omitir correla√ß√µes nulas  
**Exemplo:** "Temperatura correlaciona (r=0.3)" mas omitir "Bateria N√ÉO correlaciona (r=0.05)"

**Checklist de Mitiga√ß√£o:**
- [ ] Criar se√ß√£o "Fatores Testados": Listar **todos** fatores analisados
- [ ] Reportar correla√ß√µes nulas: "Bateria: r=0.05, p=0.4 (N√ÉO significativo)"
- [ ] Documentar: "Testamos 15 correla√ß√µes, apenas 4 foram significativas"
- [ ] Evitar omitir resultados negativos - s√£o t√£o importantes quanto positivos
- [ ] Adicionar se√ß√£o: "O que N√ÉO correlaciona" (insights por nega√ß√£o)

#### 7.2 P-Hacking (Multiple Comparisons)

**Problema:** Testar 100 correla√ß√µes e reportar apenas as 5 com p<0.05  
**Exemplo:** Com 100 testes, 5 p<0.05 aparecem **por acaso** (false positives)

**Checklist de Mitiga√ß√£o:**
- [ ] Aplicar corre√ß√£o de Bonferroni: p_adjusted = p_raw √ó n_comparisons
- [ ] Documentar: "Testamos X correla√ß√µes, aplicamos corre√ß√£o Bonferroni"
- [ ] Usar threshold mais rigoroso: p < 0.01 (n√£o p < 0.05)
- [ ] Validar correla√ß√µes em holdout set (dados de Nov/2025 em diante)
- [ ] Evitar data dredging: Definir hip√≥teses **antes** de testar

#### 7.3 Visualiza√ß√µes Enganosas

**Problema:** Escala de eixo Y manipulada para exagerar diferen√ßas  
**Exemplo:** Gr√°fico de barras com eixo Y come√ßando em 90% (n√£o 0%)

**Checklist de Mitiga√ß√£o:**
- [ ] Sempre come√ßar eixo Y em zero para gr√°ficos de barras
- [ ] Adicionar linha de refer√™ncia: "M√©dia geral" ou "Baseline"
- [ ] Documentar escala: "Eixo Y: 0-100% (escala completa)"
- [ ] Evitar truncar eixos sem justificativa clara
- [ ] Usar visualiza√ß√µes honestas: boxplot mostra distribui√ß√£o completa, n√£o apenas m√©dia

---

## 8. Vi√©s de Interpreta√ß√£o (Interpretation Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**ALTO** - Stakeholders n√£o-t√©cnicos interpretar√£o resultados

### üìå Manifesta√ß√µes Poss√≠veis

#### 8.1 Correla√ß√£o ‚â† Causa√ß√£o

**Problema:** Stakeholder v√™ r=-0.22 entre RSSI e msg6_rate e conclui "RSSI causa falhas"  
**Exemplo:** Correla√ß√£o pode ser mediada por terceira vari√°vel (temperatura afeta RSSI E chip)

**Checklist de Mitiga√ß√£o:**
- [ ] Adicionar disclaimer SEMPRE: "Correla√ß√£o n√£o implica causa√ß√£o"
- [ ] Usar linguagem precisa: "RSSI correlaciona com falhas" (n√£o "causa")
- [ ] Criar diagramas causais quando poss√≠vel (temperatura ‚Üí RSSI ‚Üí falhas)
- [ ] Documentar confounders conhecidos: "Temperatura pode influenciar ambos"
- [ ] Evitar implicar causa√ß√£o em t√≠tulos de gr√°ficos

#### 8.2 "Falso Positivo" vs "Falha Real"

**Problema:** msg_type 6 pode ser alarme falso (device funcionando mas reportou erro)  
**Exemplo:** Device reporta CHIP_FAIL mas continua funcionando normalmente ap√≥s reset

**Checklist de Mitiga√ß√£o:**
- [ ] Adicionar m√©trica: "Taxa de auto-recupera√ß√£o" por tipo de erro
- [ ] Documentar: "Error code 7 tem 80% auto-recupera√ß√£o ‚Üí prov√°vel falso positivo"
- [ ] Criar classifica√ß√£o: "Falhas permanentes" vs "Falhas transit√≥rias"
- [ ] Evitar alarmar cliente com msg_type 6 que se auto-corrigem
- [ ] Adicionar contexto: "X% deste tipo de erro se resolve automaticamente"

#### 8.3 "Signific√¢ncia Estat√≠stica" vs "Relev√¢ncia Pr√°tica"

**Problema:** p<0.001 mas r=0.05 ‚Üí Estatisticamente significativo mas praticamente irrelevante  
**Exemplo:** "Temperatura correlaciona significativamente (p<0.001)" mas explica apenas 0.25% da vari√¢ncia

**Checklist de Mitiga√ß√£o:**
- [ ] Sempre reportar **tamanho do efeito** (r, r¬≤) junto com p-value
- [ ] Documentar: "p<0.001 MAS r¬≤=0.0025 (0.25% vari√¢ncia explicada)"
- [ ] Usar threshold pr√°tico: "Correla√ß√£o relevante: |r| > 0.3 OU r¬≤ > 10%"
- [ ] Evitar enfatizar p-value sem contexto de magnitude
- [ ] Adicionar interpreta√ß√£o: "Estatisticamente significativo mas efeito FRACO"

---

## 9. Vi√©s de Medi√ß√£o (Measurement Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**M√âDIO** - Telemetrias podem ter erros de medi√ß√£o ou calibra√ß√£o

### üìå Manifesta√ß√µes Poss√≠veis

#### 9.1 Precis√£o de Sensores

**Problema:** Sensor de temperatura pode ter erro ¬±2¬∞C  
**Exemplo:** Correla√ß√£o temperatura √ó falhas pode ser ru√≠do de medi√ß√£o

**Checklist de Mitiga√ß√£o:**
- [ ] Documentar precis√£o de sensores: "Temperatura: ¬±2¬∞C, Bateria: ¬±0.01V"
- [ ] Calcular SNR (Signal-to-Noise Ratio) de medi√ß√µes
- [ ] Filtrar outliers √≥bvios: "Temperatura >100¬∞C ou <-40¬∞C ‚Üí prov√°vel erro"
- [ ] Adicionar intervalo de confian√ßa em visualiza√ß√µes
- [ ] Evitar interpretar correla√ß√µes fracas em medi√ß√µes ruidosas

#### 9.2 Timestamp Accuracy

**Problema:** @timestamp pode ter drift (rel√≥gio do device dessincronizado)  
**Exemplo:** "Pico de falhas √†s 3h AM" pode ser artefato de timezone ou drift

**Checklist de Mitiga√ß√£o:**
- [ ] Verificar consist√™ncia de timestamps: "Todos timestamps em GMT? UTC?"
- [ ] Calcular gap entre mensagens consecutivas: "Gaps >24h indicam poss√≠vel drift"
- [ ] Documentar timezone: "Timestamps em GMT-3 (hor√°rio Bras√≠lia)"
- [ ] Filtrar eventos com timestamps imposs√≠veis (futuro ou 1970)
- [ ] Adicionar warning: "Precis√£o temporal ¬±5 minutos"

#### 9.3 Missing Data Patterns

**Problema:** Dados faltantes podem n√£o ser aleat√≥rios (MNAR - Missing Not At Random)  
**Exemplo:** RSSI ausente apenas quando sinal est√° MUITO fraco (device n√£o consegue enviar)

**Checklist de Mitiga√ß√£o:**
- [ ] Testar se missing values s√£o aleat√≥rios: "Little's MCAR test"
- [ ] Comparar caracter√≠sticas de devices com/sem telemetria
- [ ] Documentar: "RSSI ausente em X% dos casos - prov√°vel vi√©s de medi√ß√£o"
- [ ] Evitar imputar valores faltantes sem entender mecanismo de aus√™ncia
- [ ] Adicionar an√°lise: "Devices sem RSSI t√™m taxa de msg6 Y% maior"

---

## 10. Vi√©s Operacional (Operational Bias)

### ‚ö†Ô∏è Risco para Este Projeto

**ALTO** - Dashboard influenciar√° decis√µes operacionais

### üìå Manifesta√ß√µes Poss√≠veis

#### 10.1 Profecia Auto-Realiz√°vel

**Problema:** Dashboard mostra "Device X em risco" ‚Üí T√©cnico troca device ‚Üí "Predi√ß√£o confirmada"  
**Exemplo:** Device poderia ter se auto-recuperado, mas foi trocado preventivamente

**Checklist de Mitiga√ß√£o:**
- [ ] Documentar: "Recomenda√ß√£o N√ÉO √© predi√ß√£o - device pode se auto-recuperar"
- [ ] Adicionar m√©trica: "Taxa de auto-recupera√ß√£o hist√≥rica: X%"
- [ ] Criar protocolo: "Aguardar 24h antes de dispatch se error code Y"
- [ ] Evitar linguagem definitiva: "Device falhar√°" ‚Üí Usar: "Device apresenta padr√£o Z"
- [ ] Trackear interven√ß√µes: Registrar se device foi trocado ou se auto-recuperou

#### 10.2 Otimiza√ß√£o Prematura

**Problema:** Stakeholder v√™ correla√ß√£o r=-0.2 e decide investir em solu√ß√£o cara  
**Exemplo:** "RSSI correlaciona ‚Üí Vamos comprar 1000 amplificadores de sinal"

**Checklist de Mitiga√ß√£o:**
- [ ] Adicionar an√°lise de ROI: "RSSI explica 4% das falhas - amplificador reduz 4% de dispatches?"
- [ ] Documentar custo-benef√≠cio: "Investimento $X para redu√ß√£o de Y% falhas"
- [ ] Recomendar piloto: "Testar amplificador em 10 devices antes de escalar"
- [ ] Evitar implicar que solu√ß√£o √∫nica resolver√° problema complexo
- [ ] Adicionar se√ß√£o: "Outros fatores a considerar" (temperatura, bateria, etc)

#### 10.3 Tunnel Vision

**Problema:** Dashboard foca em msg_type 6 ‚Üí Time ignora outros problemas  
**Exemplo:** Device sem msg_type 6 mas com bateria cr√≠tica √© negligenciado

**Checklist de Mitiga√ß√£o:**
- [ ] Adicionar se√ß√£o: "Devices sem msg_type 6 mas com risco" (bateria <2.5V, etc)
- [ ] Criar alertas para m√∫ltiplos indicadores, n√£o s√≥ msg_type 6
- [ ] Documentar: "msg_type 6 √© apenas 1 indicador - verificar bateria, RSSI, temperatura"
- [ ] Evitar criar incentivo perverso: "Zero msg_type 6 = sucesso" (device pode estar morto)
- [ ] Adicionar m√©trica de sa√∫de hol√≠stica: "Health Score = f(msg6, bateria, RSSI, uptime)"

---

## ‚úÖ CHECKLIST DE VALIDA√á√ÉO FINAL

### Antes de Publicar Dashboard

- [ ] Todas visualiza√ß√µes t√™m t√≠tulo claro e descritivo
- [ ] Todos eixos t√™m labels com unidades (¬∞C, dBm, %, etc)
- [ ] Disclaimers adicionados: "Correla√ß√£o ‚â† Causa√ß√£o"
- [ ] Limita√ß√µes documentadas: "Dataset cobre apenas Jan-Out 2025, rede VIVO"
- [ ] Resultados negativos reportados: "O que N√ÉO correlaciona"
- [ ] P-values ajustados para m√∫ltiplas compara√ß√µes (Bonferroni)
- [ ] Tamanho do efeito (r, r¬≤) reportado junto com p-value
- [ ] Intervalos de confian√ßa adicionados em gr√°ficos
- [ ] Se√ß√£o "Como Interpretar" para stakeholders n√£o-t√©cnicos
- [ ] Contact info para reportar bugs ou questionar resultados

### Antes de Apresentar para Stakeholders

- [ ] Preparar slide: "Limita√ß√µes desta An√°lise"
- [ ] Preparar slide: "O que N√ÉO podemos concluir"
- [ ] Preparar resposta: "Como validamos esses padr√µes?"
- [ ] Preparar protocolo: "Como agir baseado nestes insights?"
- [ ] Ter an√°lise de sensibilidade: "E se removermos outliers?"
- [ ] Ter an√°lise de robustez: "Padr√£o se mant√©m em subgrupos?"

### Monitoramento Cont√≠nuo (P√≥s-Deploy)

- [ ] Criar alerta: "Correla√ß√£o mudou >30% no √∫ltimo m√™s"
- [ ] Criar alerta: "Novo padr√£o detectado (n√£o visto antes)"
- [ ] Criar alerta: "Dataset cresceu >20% - re-validar an√°lises"
- [ ] Documentar decis√µes tomadas baseadas em dashboard
- [ ] Trackear outcome: "Recomenda√ß√£o X levou a resultado Y?"
- [ ] Revisar checklist mensalmente: "Novos vieses identificados?"

---

## üìö REFER√äNCIAS E RECURSOS

### Livros Recomendados

1. **"Thinking, Fast and Slow"** - Daniel Kahneman (vieses cognitivos)
2. **"The Book of Why"** - Judea Pearl (causalidade)
3. **"Trustworthy Online Controlled Experiments"** - Kohavi et al. (A/B testing, p-hacking)

### Papers Relevantes

1. **Simpson's Paradox** - IEEE Transactions on Knowledge and Data Engineering
2. **Missing Data Mechanisms** - Little & Rubin (MCAR, MAR, MNAR)
3. **Multiple Testing Corrections** - Bonferroni, Benjamini-Hochberg

### Ferramentas

1. **Scipy.stats** - Testes estat√≠sticos (spearmanr, pearsonr, ttest_ind)
2. **Statsmodels** - Little's MCAR test, decomposi√ß√£o STL
3. **Plotly** - Visualiza√ß√µes interativas com intervalos de confian√ßa

---

## üéØ PRINC√çPIO GUIA

> **"N√£o busque confirmar o que voc√™ acha que sabe.  
> Busque REFUTAR o que voc√™ acha que sabe.  
> O que sobreviver √† refuta√ß√£o √© conhecimento robusto."**

---

**Bom trabalho! üõ°Ô∏è**  
Use este checklist como **guia vivo** - adicione novos vieses conforme identificados.

---

_Checklist criado: 03/Nov/2025_  
_√öltima atualiza√ß√£o: 03/Nov/2025_  
_Status: ATIVO - Revisar mensalmente_
